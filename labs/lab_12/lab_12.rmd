---
title: "lab_12"
author: "derek willis"
date: "2023-05-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

* tidytext and our usual libraries

## Load libraries and establish settings

**Task** Create a codeblock and load appropriate packages and settings for this lab.

```{r}
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
library(rvest)
```





## Questions

**Q1.** You've been assigned to report a story about the leading reasons that Maryland attorneys get sanctioned by the state for misconduct. The state [publishes lists of sanctions](https://www.courts.state.md.us/attygrievance/sanctions) that contain a short text description about the situation. Load the CSV file in the data folder containing records from fiscal year 2011 onwards.

#1. Make a list of unique words from the text column, then following the example in the pre_lab, 

#2 remove common "stop words" from that list 

#3 and create a list of the top 10 words containing the percentage of occurrences each word represents. 

What's the leading word in that answer and, broadly, what do you think the top 10 words describe?

**A1.**The leading word in this question is "failing." I believe the first 10 words represent the positions of people featured on Maryland's sanctions website. In addition, I believe the words also accurately describe terms involving the court system. For example; The words used were as following attorney, client, trust, and many more.
```{r}
# load the CSV
sanctions <- read_csv("data/md_attorney_sanctions.csv")
```
```{r}
a_list_of_words <- c("Commission", "Reprimand", "Indefinite", "Disbarred", "Suspension", ",")
unique(a_list_of_words)
```

```{r}
uniquewords_ <- sanctions %>% select(text) %>%
  unnest_tokens(word, text)
View(uniquewords_)
```

```{r}
# remove common words from list
data("stop_words")

uniquewords_ %>%
  anti_join(stop_words) %>%
  group_by(word) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(50)
```




**Q2.** Let's move beyond single words to phrases. Make a list of the top 10 three-word phrases, called trigrams, based on the example from the pre_lab (you'll need to modify the example code to do this). What's the top trigram and how often does it appear? What does that phrase mean in legal terms?

**A2.**The top trigram is attorney trust account and it appears 343 times. According to Smokeball.com, an attorney trust account is a particular bank account where client funds are kept safe until the case has concluded. For example: if a client is in a case, it would be considered good business for their funds to be held in a attorney trust account until it's time to compensate the attorney.
```{r}
sanctions %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 3) %>%
  separate(bigram, c("word1", "word2","word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  mutate(bigram = paste(word1, word2, word3, sep=" ")) %>%
  group_by(bigram) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)
```

**Q3.** Let's drop back down to more traditional text analysis - take the top trigram from Q2 and write code to see how many times it occurs in the text column in each fiscal year. What do you think the answer produced by your code suggests? What else could you do to try and clarify the most important reasons attorneys get sanctioned?

**A3.**I think the answer produced the first 10 entries belonging to David Ollie Caplan in 2023. Also, it could also suggests Caplan has been deemed "dishonest" and happened in "2022." I could try and see if Caplan was sanctioned for fraud based on its pair of appearances in the data.There's 5,928 total rows in the data and could used with its associated terms to possible discover the reason for being sanctioned.

```{r}
sanctions %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 3) %>%
  separate(bigram, c("word1", "word2","word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)
```
